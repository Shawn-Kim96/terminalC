Loading model: data/model/meta-llama/llama-3.1-8B-instruct
Using device: cuda
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695
Loading dataset from data/training_data.jsonl

=== Training Configuration ===
Dataset size: 23
Batch size: 1
Gradient accumulation steps: 1
Effective batch size: 1
Number of epochs: 100
Learning rate: 0.0002
Loss will be logged to: models/small_model_lora/training_loss.csv
Loss calculation: Standard Causal Language Modeling (next token prediction)
==============================

Starting training...
Epoch 1.00: Loss = 1.5294
{'loss': 1.5294, 'grad_norm': 3.111182689666748, 'learning_rate': 0.00019843478260869567, 'epoch': 1.0}
Epoch 2.00: Loss = 0.5512
{'loss': 0.5512, 'grad_norm': 3.2742295265197754, 'learning_rate': 0.00019643478260869565, 'epoch': 2.0}
Epoch 3.00: Loss = 0.3634
{'loss': 0.3634, 'grad_norm': 3.2869880199432373, 'learning_rate': 0.00019443478260869565, 'epoch': 3.0}
Epoch 4.00: Loss = 0.2456
{'loss': 0.2456, 'grad_norm': 2.9674642086029053, 'learning_rate': 0.00019243478260869566, 'epoch': 4.0}
Epoch 5.00: Loss = 0.1545
{'loss': 0.1545, 'grad_norm': 0.8152027130126953, 'learning_rate': 0.00019043478260869566, 'epoch': 5.0}
Epoch 6.00: Loss = 0.1136
{'loss': 0.1136, 'grad_norm': 3.374770164489746, 'learning_rate': 0.00018843478260869567, 'epoch': 6.0}
Epoch 7.00: Loss = 0.0952
{'loss': 0.0952, 'grad_norm': 0.5740269422531128, 'learning_rate': 0.00018643478260869567, 'epoch': 7.0}
Epoch 8.00: Loss = 0.0827
{'loss': 0.0827, 'grad_norm': 2.7321617603302, 'learning_rate': 0.00018443478260869568, 'epoch': 8.0}
Epoch 9.00: Loss = 0.0692
{'loss': 0.0692, 'grad_norm': 0.4638543426990509, 'learning_rate': 0.00018243478260869566, 'epoch': 9.0}
Epoch 10.00: Loss = 0.0551
{'loss': 0.0551, 'grad_norm': 0.8460745215415955, 'learning_rate': 0.00018043478260869566, 'epoch': 10.0}
Epoch 11.00: Loss = 0.0766
{'loss': 0.0766, 'grad_norm': 0.2752002775669098, 'learning_rate': 0.00017843478260869567, 'epoch': 11.0}
Epoch 12.00: Loss = 0.0443
{'loss': 0.0443, 'grad_norm': 0.9147430658340454, 'learning_rate': 0.00017643478260869565, 'epoch': 12.0}
Epoch 13.00: Loss = 0.0455
{'loss': 0.0455, 'grad_norm': 0.4131369888782501, 'learning_rate': 0.00017443478260869565, 'epoch': 13.0}
Epoch 14.00: Loss = 0.0482
{'loss': 0.0482, 'grad_norm': 0.2340511977672577, 'learning_rate': 0.00017243478260869566, 'epoch': 14.0}
Epoch 15.00: Loss = 0.0581
{'loss': 0.0581, 'grad_norm': 0.326242059469223, 'learning_rate': 0.00017043478260869566, 'epoch': 15.0}
Epoch 16.00: Loss = 0.0450
{'loss': 0.045, 'grad_norm': 0.22706009447574615, 'learning_rate': 0.00016843478260869564, 'epoch': 16.0}
Epoch 17.00: Loss = 0.0312
{'loss': 0.0312, 'grad_norm': 0.2347477525472641, 'learning_rate': 0.00016643478260869565, 'epoch': 17.0}
Epoch 18.00: Loss = 0.0261
{'loss': 0.0261, 'grad_norm': 0.45027056336402893, 'learning_rate': 0.00016443478260869568, 'epoch': 18.0}
Epoch 19.00: Loss = 0.0239
{'loss': 0.0239, 'grad_norm': 0.1793440282344818, 'learning_rate': 0.00016243478260869566, 'epoch': 19.0}
Epoch 20.00: Loss = 0.0224
{'loss': 0.0224, 'grad_norm': 2.176732301712036, 'learning_rate': 0.00016043478260869567, 'epoch': 20.0}
Epoch 21.00: Loss = 0.0209
{'loss': 0.0209, 'grad_norm': 0.14339138567447662, 'learning_rate': 0.00015843478260869567, 'epoch': 21.0}
Epoch 22.00: Loss = 0.0219
{'loss': 0.0219, 'grad_norm': 0.10623643547296524, 'learning_rate': 0.00015643478260869565, 'epoch': 22.0}
Epoch 23.00: Loss = 0.0207
{'loss': 0.0207, 'grad_norm': 0.14830981194972992, 'learning_rate': 0.00015443478260869565, 'epoch': 23.0}
Epoch 24.00: Loss = 0.0216
{'loss': 0.0216, 'grad_norm': 0.09035944938659668, 'learning_rate': 0.00015243478260869566, 'epoch': 24.0}
Epoch 25.00: Loss = 0.0212
{'loss': 0.0212, 'grad_norm': 0.12995542585849762, 'learning_rate': 0.00015043478260869567, 'epoch': 25.0}
Epoch 26.00: Loss = 0.0251
{'loss': 0.0251, 'grad_norm': 0.16851571202278137, 'learning_rate': 0.00014843478260869564, 'epoch': 26.0}
Epoch 27.00: Loss = 0.0204
{'loss': 0.0204, 'grad_norm': 0.1254451721906662, 'learning_rate': 0.00014643478260869565, 'epoch': 27.0}
Epoch 28.00: Loss = 0.0255
{'loss': 0.0255, 'grad_norm': 0.16544680297374725, 'learning_rate': 0.00014443478260869566, 'epoch': 28.0}
Epoch 29.00: Loss = 0.0272
{'loss': 0.0272, 'grad_norm': 0.1312861144542694, 'learning_rate': 0.00014243478260869566, 'epoch': 29.0}
Epoch 30.00: Loss = 0.0231
{'loss': 0.0231, 'grad_norm': 1.4645062685012817, 'learning_rate': 0.00014043478260869567, 'epoch': 30.0}
Epoch 31.00: Loss = 0.0225
{'loss': 0.0225, 'grad_norm': 0.14843031764030457, 'learning_rate': 0.00013843478260869567, 'epoch': 31.0}
Epoch 32.00: Loss = 0.0223
{'loss': 0.0223, 'grad_norm': 0.1470937728881836, 'learning_rate': 0.00013643478260869568, 'epoch': 32.0}
Epoch 33.00: Loss = 0.0231
{'loss': 0.0231, 'grad_norm': 1.603600263595581, 'learning_rate': 0.00013443478260869566, 'epoch': 33.0}
Epoch 34.00: Loss = 0.0250
{'loss': 0.025, 'grad_norm': 0.15470869839191437, 'learning_rate': 0.00013243478260869566, 'epoch': 34.0}
Epoch 35.00: Loss = 0.0377
{'loss': 0.0377, 'grad_norm': 0.8393881916999817, 'learning_rate': 0.00013043478260869567, 'epoch': 35.0}
Epoch 36.00: Loss = 0.0214
{'loss': 0.0214, 'grad_norm': 0.11054742336273193, 'learning_rate': 0.00012843478260869565, 'epoch': 36.0}
Epoch 37.00: Loss = 0.0211
{'loss': 0.0211, 'grad_norm': 0.16353505849838257, 'learning_rate': 0.00012643478260869565, 'epoch': 37.0}
Epoch 38.00: Loss = 0.0236
{'loss': 0.0236, 'grad_norm': 0.09811458736658096, 'learning_rate': 0.00012443478260869566, 'epoch': 38.0}
Epoch 39.00: Loss = 0.0187
{'loss': 0.0187, 'grad_norm': 0.1290494054555893, 'learning_rate': 0.00012243478260869566, 'epoch': 39.0}
Epoch 40.00: Loss = 0.0185
{'loss': 0.0185, 'grad_norm': 0.14079861342906952, 'learning_rate': 0.00012043478260869566, 'epoch': 40.0}
Epoch 41.00: Loss = 0.0220
{'loss': 0.022, 'grad_norm': 2.027759552001953, 'learning_rate': 0.00011843478260869565, 'epoch': 41.0}
Epoch 42.00: Loss = 0.0219
{'loss': 0.0219, 'grad_norm': 0.08259107917547226, 'learning_rate': 0.00011643478260869565, 'epoch': 42.0}
Epoch 43.00: Loss = 0.0203
{'loss': 0.0203, 'grad_norm': 0.10068865120410919, 'learning_rate': 0.00011443478260869567, 'epoch': 43.0}
Epoch 44.00: Loss = 0.0180
{'loss': 0.018, 'grad_norm': 0.11110841482877731, 'learning_rate': 0.00011243478260869566, 'epoch': 44.0}
Epoch 45.00: Loss = 0.0182
{'loss': 0.0182, 'grad_norm': 0.10922467708587646, 'learning_rate': 0.00011043478260869567, 'epoch': 45.0}
Epoch 46.00: Loss = 0.0180
{'loss': 0.018, 'grad_norm': 0.09180033206939697, 'learning_rate': 0.00010843478260869566, 'epoch': 46.0}
Epoch 47.00: Loss = 0.0178
{'loss': 0.0178, 'grad_norm': 0.08615735173225403, 'learning_rate': 0.00010643478260869565, 'epoch': 47.0}
Epoch 48.00: Loss = 0.0178
{'loss': 0.0178, 'grad_norm': 0.11106232553720474, 'learning_rate': 0.00010443478260869566, 'epoch': 48.0}
Epoch 49.00: Loss = 0.0179
{'loss': 0.0179, 'grad_norm': 0.10063855350017548, 'learning_rate': 0.00010243478260869565, 'epoch': 49.0}
Epoch 50.00: Loss = 0.0178
{'loss': 0.0178, 'grad_norm': 0.12370160222053528, 'learning_rate': 0.00010043478260869566, 'epoch': 50.0}
Epoch 51.00: Loss = 0.0179
{'loss': 0.0179, 'grad_norm': 0.09803371131420135, 'learning_rate': 9.843478260869565e-05, 'epoch': 51.0}
Epoch 52.00: Loss = 0.0176
{'loss': 0.0176, 'grad_norm': 0.11260763555765152, 'learning_rate': 9.643478260869567e-05, 'epoch': 52.0}
Epoch 53.00: Loss = 0.0176
{'loss': 0.0176, 'grad_norm': 0.18619996309280396, 'learning_rate': 9.443478260869566e-05, 'epoch': 53.0}
Epoch 54.00: Loss = 0.0175
{'loss': 0.0175, 'grad_norm': 0.11881394684314728, 'learning_rate': 9.243478260869565e-05, 'epoch': 54.0}
Epoch 55.00: Loss = 0.0176
{'loss': 0.0176, 'grad_norm': 0.11999468505382538, 'learning_rate': 9.043478260869566e-05, 'epoch': 55.0}
Epoch 56.00: Loss = 0.0175
{'loss': 0.0175, 'grad_norm': 0.10863304883241653, 'learning_rate': 8.843478260869565e-05, 'epoch': 56.0}
Epoch 57.00: Loss = 0.0181
{'loss': 0.0181, 'grad_norm': 0.09464100003242493, 'learning_rate': 8.643478260869566e-05, 'epoch': 57.0}
Epoch 58.00: Loss = 0.0175
{'loss': 0.0175, 'grad_norm': 0.12010768800973892, 'learning_rate': 8.443478260869566e-05, 'epoch': 58.0}
Epoch 59.00: Loss = 0.0174
{'loss': 0.0174, 'grad_norm': 0.10762753337621689, 'learning_rate': 8.243478260869565e-05, 'epoch': 59.0}
Epoch 60.00: Loss = 0.0174
{'loss': 0.0174, 'grad_norm': 0.1206747367978096, 'learning_rate': 8.043478260869566e-05, 'epoch': 60.0}
Epoch 61.00: Loss = 0.0173
{'loss': 0.0173, 'grad_norm': 0.14995886385440826, 'learning_rate': 7.843478260869565e-05, 'epoch': 61.0}
Epoch 62.00: Loss = 0.0177
{'loss': 0.0177, 'grad_norm': 0.13785791397094727, 'learning_rate': 7.643478260869566e-05, 'epoch': 62.0}
Epoch 63.00: Loss = 0.0171
{'loss': 0.0171, 'grad_norm': 0.0861935093998909, 'learning_rate': 7.443478260869565e-05, 'epoch': 63.0}
Epoch 64.00: Loss = 0.0179
{'loss': 0.0179, 'grad_norm': 0.12404227256774902, 'learning_rate': 7.243478260869565e-05, 'epoch': 64.0}
Epoch 65.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.12290586531162262, 'learning_rate': 7.043478260869566e-05, 'epoch': 65.0}
Epoch 66.00: Loss = 0.0174
{'loss': 0.0174, 'grad_norm': 0.09663815051317215, 'learning_rate': 6.843478260869565e-05, 'epoch': 66.0}
Epoch 67.00: Loss = 0.0172
{'loss': 0.0172, 'grad_norm': 0.1331128031015396, 'learning_rate': 6.643478260869566e-05, 'epoch': 67.0}
Epoch 68.00: Loss = 0.0172
{'loss': 0.0172, 'grad_norm': 0.07705593854188919, 'learning_rate': 6.443478260869565e-05, 'epoch': 68.0}
Epoch 69.00: Loss = 0.0173
{'loss': 0.0173, 'grad_norm': 0.12213140726089478, 'learning_rate': 6.243478260869565e-05, 'epoch': 69.0}
Epoch 70.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.10963287204504013, 'learning_rate': 6.0434782608695654e-05, 'epoch': 70.0}
Epoch 71.00: Loss = 0.0169
{'loss': 0.0169, 'grad_norm': 0.09534092247486115, 'learning_rate': 5.843478260869566e-05, 'epoch': 71.0}
Epoch 72.00: Loss = 0.0172
{'loss': 0.0172, 'grad_norm': 0.09965184330940247, 'learning_rate': 5.643478260869566e-05, 'epoch': 72.0}
Epoch 73.00: Loss = 0.0167
{'loss': 0.0167, 'grad_norm': 0.10295429080724716, 'learning_rate': 5.443478260869566e-05, 'epoch': 73.0}
Epoch 74.00: Loss = 0.0170
{'loss': 0.017, 'grad_norm': 0.14510126411914825, 'learning_rate': 5.2434782608695656e-05, 'epoch': 74.0}
Epoch 75.00: Loss = 0.0170
{'loss': 0.017, 'grad_norm': 0.1283903867006302, 'learning_rate': 5.0434782608695655e-05, 'epoch': 75.0}
Epoch 76.00: Loss = 0.0169
{'loss': 0.0169, 'grad_norm': 0.14489004015922546, 'learning_rate': 4.843478260869565e-05, 'epoch': 76.0}
Epoch 77.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.11053091287612915, 'learning_rate': 4.643478260869565e-05, 'epoch': 77.0}
Epoch 78.00: Loss = 0.0173
{'loss': 0.0173, 'grad_norm': 0.08465149253606796, 'learning_rate': 4.443478260869565e-05, 'epoch': 78.0}
Epoch 79.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.07971598207950592, 'learning_rate': 4.2434782608695657e-05, 'epoch': 79.0}
Epoch 80.00: Loss = 0.0170
{'loss': 0.017, 'grad_norm': 0.10606145858764648, 'learning_rate': 4.0434782608695655e-05, 'epoch': 80.0}
Epoch 81.00: Loss = 0.0173
{'loss': 0.0173, 'grad_norm': 0.11910570412874222, 'learning_rate': 3.8434782608695654e-05, 'epoch': 81.0}
Epoch 82.00: Loss = 0.0173
{'loss': 0.0173, 'grad_norm': 0.09481170773506165, 'learning_rate': 3.643478260869565e-05, 'epoch': 82.0}
Epoch 83.00: Loss = 0.0171
{'loss': 0.0171, 'grad_norm': 0.09352238476276398, 'learning_rate': 3.443478260869565e-05, 'epoch': 83.0}
Epoch 84.00: Loss = 0.0166
{'loss': 0.0166, 'grad_norm': 0.07438290119171143, 'learning_rate': 3.243478260869565e-05, 'epoch': 84.0}
Epoch 85.00: Loss = 0.0170
{'loss': 0.017, 'grad_norm': 0.11324600130319595, 'learning_rate': 3.0434782608695656e-05, 'epoch': 85.0}
Epoch 86.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.2476022094488144, 'learning_rate': 2.8434782608695655e-05, 'epoch': 86.0}
Epoch 87.00: Loss = 0.0171
{'loss': 0.0171, 'grad_norm': 0.09285786747932434, 'learning_rate': 2.643478260869565e-05, 'epoch': 87.0}
Epoch 88.00: Loss = 0.0167
{'loss': 0.0167, 'grad_norm': 0.09122113883495331, 'learning_rate': 2.4434782608695653e-05, 'epoch': 88.0}
Epoch 89.00: Loss = 0.0166
{'loss': 0.0166, 'grad_norm': 0.12117607146501541, 'learning_rate': 2.2434782608695655e-05, 'epoch': 89.0}
Epoch 90.00: Loss = 0.0169
{'loss': 0.0169, 'grad_norm': 0.1325702965259552, 'learning_rate': 2.0434782608695654e-05, 'epoch': 90.0}
Epoch 91.00: Loss = 0.0164
{'loss': 0.0164, 'grad_norm': 0.09688602387905121, 'learning_rate': 1.8434782608695653e-05, 'epoch': 91.0}
Epoch 92.00: Loss = 0.0167
{'loss': 0.0167, 'grad_norm': 0.0869792178273201, 'learning_rate': 1.6434782608695655e-05, 'epoch': 92.0}
Epoch 93.00: Loss = 0.0169
{'loss': 0.0169, 'grad_norm': 0.09862479567527771, 'learning_rate': 1.4434782608695652e-05, 'epoch': 93.0}
Epoch 94.00: Loss = 0.0159
{'loss': 0.0159, 'grad_norm': 0.1375173032283783, 'learning_rate': 1.2434782608695652e-05, 'epoch': 94.0}
Epoch 95.00: Loss = 0.0168
{'loss': 0.0168, 'grad_norm': 0.09577339142560959, 'learning_rate': 1.0434782608695651e-05, 'epoch': 95.0}
Epoch 96.00: Loss = 0.0162
{'loss': 0.0162, 'grad_norm': 0.10137385129928589, 'learning_rate': 8.434782608695653e-06, 'epoch': 96.0}
Epoch 97.00: Loss = 0.0161
{'loss': 0.0161, 'grad_norm': 0.13841795921325684, 'learning_rate': 6.434782608695652e-06, 'epoch': 97.0}
Epoch 98.00: Loss = 0.0166
{'loss': 0.0166, 'grad_norm': 0.08567336946725845, 'learning_rate': 4.434782608695652e-06, 'epoch': 98.0}
Epoch 99.00: Loss = 0.0161
{'loss': 0.0161, 'grad_norm': 0.0892307385802269, 'learning_rate': 2.434782608695652e-06, 'epoch': 99.0}
Epoch 100.00: Loss = 0.0160
{'loss': 0.016, 'grad_norm': 0.1152615025639534, 'learning_rate': 4.347826086956522e-07, 'epoch': 100.0}
{'train_runtime': 691.8541, 'train_samples_per_second': 3.324, 'train_steps_per_second': 3.324, 'train_loss': 0.05183610450962316, 'epoch': 100.0}
Saving loss history to models/small_model_lora/training_loss.csv
Saving model to models/small_model_lora
